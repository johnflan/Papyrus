Over the last number of years distributed key-value stores have become an important building block for organisations creating `internet scale' products. Companies such as Google, Amazon, Facebook and Linkedin have successfully deployed highly distributed storage systems to extract maximum performance from commodity hardware in order to provide cost effective large scale storage.

Today numerous distributed key-value stores exist with Apache Cassandra, Project Voldemort and Riak being the most prominent open source projects. These systems are designed to be massively scalable and reliable in the face of failure using distribution and replication over a large number of nodes. 

Most discussions of key value stores describe deployment only to a single site. Such assumptions enable system designers to tune the configuration to make use of high bandwidth low latency links between servers and racks. Enabling the stored data to become consistent  more quickly while minimising the number of replicas. However to offer a truly `internet scale' system these data stores must be capable of scaling across a number of data centres. Scaling across sites is necessary to provide the storage and computational resources as well as minimising communication latencies and maximising service availability.

This project intends to investigate the operation of key-value stores distributed over a number of sites. This distribution may be achieved with partitioning, replicating or a mixture of both much like the use of RAID levels for hard drives. Decisions relating to the partitioning and replication across multiple geographic regions have different considerations. For example we must consider locality to the users, legal requirements in addition to consistency and replication. The comparative bandwidth and latencies of the data links between inter-site and intra-site provide challenges in designing an effective mechanism. 
